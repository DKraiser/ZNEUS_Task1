# -*- coding: utf-8 -*-
"""speeddating.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CITJKI_UCruUVzG7FEUmWZGD00PktC3M
"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
import os
import re
import torch
import torch.nn as nn
import tqdm
from dataclasses import dataclass
import matplotlib.pyplot as plt
import wandb
import json

"""### EDA and data preparation"""

def map_field(field):
    if pd.isna(field):
        return 'Unknown'
    field = field.lower()
    if any(x in field for x in ['math', 'engineering', 'physics', 'biomed', 'computational']):
        return 'STEM'
    elif any(x in field for x in ['economics', 'finance', 'business', 'mba']):
        return 'Economics/Business'
    elif any(x in field for x in ['political', 'sociology', 'psychology', 'social work', 'international affairs', 'anthropology']):
        return 'Social Sciences'
    elif any(x in field for x in ['law', 'public policy', 'administration']):
        return 'Law/Public Policy'
    elif any(x in field for x in ['education', 'teaching', 'speech', 'tesol']):
        return 'Education'
    elif any(x in field for x in ['art', 'music', 'literature', 'creative', 'writing', 'philosophy']):
        return 'Arts/Humanities'
    elif any(x in field for x in ['medicine', 'health', 'biolog', 'epidemiology', 'nutrition', 'neuroscience']):
        return 'Health/Medicine'
    elif any(x in field for x in ['language', 'international', 'cultural']):
        return 'Language/International'
    else:
        return 'Other'

def parse_mean(s):
        if pd.isna(s):
            return None
        match = re.search(r'(\d+\.?\d*)-(\d+\.?\d*)', str(s))
        if match:
            return (float(match.group(1)) + float(match.group(2))) / 2
        else:
            return None

def preprocess(df):
  print(f'Shape of the dataset: {df.shape}')
  print()

  print(df.info())
  print()

  print(df['match'].value_counts())
  print()

  print('Missing values statistics')
  print(df.isna().sum().sort_values(ascending=False).head(30))

  # Data preprocessing
  # Ignores
  df = df.drop(columns=['decision', 'decision_o'])
  df['field'] = df['field'].fillna('Unknown').apply(map_field)

  # Default num vals
  num_cols = df.select_dtypes(include=['int64', 'float64']).drop(columns=['match']).columns
  # Ranges [a-b]
  range_cols = [col for col in df.columns if df[col].dtype == 'object' and df[col].str.contains(r'\[.*-.*\]', regex=True, na=False).any()]
  # Categorical values - all that are not in others
  categorical_cols = [col for col in df.columns if df[col].dtype == 'object' and not col in range_cols]

  print(f'Numeric cols: {len(num_cols)}')
  print(f'Range cols: {len(range_cols)}')
  print(f'Categorical cols: {len(categorical_cols)}')
  print(f'Total cols num: {len(num_cols) + len(range_cols) + len(categorical_cols)}')

  std_scaler = StandardScaler()
  df[num_cols] = df[num_cols].fillna(df[num_cols].median())
  df[num_cols] = std_scaler.fit_transform(df[num_cols])

  df[categorical_cols] = df[categorical_cols].fillna("Unknown")
  df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

  for col in range_cols:
      df[col] = df[col].apply(parse_mean)
      df[col] = df[col].fillna(df[col].median())

  df[range_cols] = std_scaler.fit_transform(df[range_cols])
  return df

def clear(df):
  FEATURE_SIMILARITY_THRESHOLD = 0.8
  MATCH_CORRELATION_LOWER_THRESHOLD = 0.1

  corr = df.corr()
  match_corr = corr['match'].sort_values(key=abs, ascending=False)
  print("Top correlations with 'match':\n", match_corr[1:].head(50))

  high_corr = []

  for i in range(len(corr.columns)):
      for j in range(i):
          if abs(corr.iloc[i, j]) > FEATURE_SIMILARITY_THRESHOLD:
              col1 = corr.columns[i]
              col2 = corr.columns[j]
              high_corr.append((col1, col2, corr.iloc[i, j]))

  sorted_data = sorted(high_corr, key=lambda tup: tup[2], reverse=True)
  for col1, col2, corr_value in sorted_data:
      print(f"{col1} and {col2} are highly correlated: {corr_value:.2f}")

  #Drop useless data
  dropped = set()
  for c1, c2, p in high_corr:
      dropped.add(c2)

  low_corr_cols = match_corr[abs(match_corr) < MATCH_CORRELATION_LOWER_THRESHOLD].index.tolist()
  for col in low_corr_cols:
      if col != 'match':
          dropped.add(col)

  return df.drop(columns=list(dropped), errors='ignore')

"""### Classes used in the project
- Data - contains the dataset
- Trainer - contains the logic of training and validating the dataset
- MLP - the NN itself
- Cfg - a storage of the params
"""

class Data:
  def __init__(self, df, cfg):
    ds_X = torch.tensor(df.drop(columns=['match']).values, dtype=torch.float32)
    ds_y = torch.tensor(df['match'].values, dtype=torch.float32).view(-1, 1)

    self.X_train, X_temp, self.y_train, y_temp = train_test_split(ds_X, ds_y, test_size=1 - cfg["training"], random_state=cfg["seed"])
    self.X_val, X_test, self.y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=cfg["seed"])

    y_train_flat = self.y_train.view(-1)
    class_counts = torch.bincount(y_train_flat.long())
    class_weights = 1.0 / class_counts.float()
    sample_weights = class_weights[y_train_flat.long()]
    sampler = torch.utils.data.WeightedRandomSampler(
        weights=sample_weights,
        num_samples=len(sample_weights),
        replacement=True
    )

    train_ds = torch.utils.data.TensorDataset(self.X_train, self.y_train)
    val_ds = torch.utils.data.TensorDataset(self.X_val, self.y_val)
    test_ds = torch.utils.data.TensorDataset(X_test, y_test)

    self.dataloader_train = torch.utils.data.DataLoader(
        train_ds,
        batch_size=cfg["batch_size"],
        num_workers=cfg["max_workers"],
        sampler=sampler,
        shuffle=False
    )

    self.dataloader_val = torch.utils.data.DataLoader(
        val_ds,
        batch_size=cfg["batch_size"],
        num_workers=cfg["max_workers"],
        shuffle=False
    )

    self.dataloader_test = torch.utils.data.DataLoader(
        test_ds,
        batch_size=cfg["batch_size"],
        num_workers=cfg["max_workers"],
        shuffle=False
    )

class MLP(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.main = nn.Sequential(
            nn.Linear(cfg["n_in"], cfg["n_hidden"]),
            nn.LeakyReLU(),
            nn.Dropout(cfg["dropout"]),
            nn.Linear(cfg["n_hidden"], cfg["n_out"]),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.main(x)

class Trainer:
  def __init__(self, df):
    self.cfg = wandb.config
    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    self.data = Data(df, self.cfg)
    self.mlp = MLP(self.cfg).to(self.device)

  def fit(self):
    loss = nn.BCELoss()
    opt = torch.optim.Adam(params=self.mlp.parameters(), lr=self.cfg["learning_rate"], weight_decay=cfg["weight_decay"])
    for epoch in range(self.cfg["epochs"]):
      log = dict()
      self.train(loss, opt, epoch, log)
      self.val(loss, epoch, log)

      wandb.log({
        "train_loss": log["train_avg_loss"],
        "val_loss": log["val_avg_loss"],
        "accuracy": log["accuracy"],
        "precision": log["precision"],
        "recall": log["recall"],
        "f1": log["f1"]
      })

  def train(self, loss, opt, epoch, log):
    total_loss = 0
    with tqdm.tqdm(self.data.dataloader_train, desc=f"Train {epoch}: ") as progress:
      for x, y in progress:
        x = x.to(self.device)
        y = y.to(self.device)

        match_hat = self.mlp(x)
        l = loss(match_hat, y)

        opt.zero_grad()
        l.backward()
        opt.step()

        total_loss += l.item()

      avg_loss = total_loss / len(self.data.dataloader_train)
      log["train_avg_loss"] = avg_loss

  def val(self, loss, epoch, log):
    total_loss = 0
    all_preds = []
    all_targets = []

    with tqdm.tqdm(self.data.dataloader_val, desc=f"Val {epoch}: ") as progress:
      with torch.no_grad():
        for x, y in progress:
          x = x.to(self.device)
          y = y.to(self.device)

          match_hat = self.mlp(x)
          l = loss(match_hat, y)

          total_loss += l.item()

          all_preds.append((match_hat.cpu() > cfg["decision_threshold"]).float())
          all_targets.append(y.cpu())

    avg_loss = total_loss / len(self.data.dataloader_val)
    all_preds = torch.cat(all_preds)
    all_targets = torch.cat(all_targets)

    TP = ((all_preds == 1) & (all_targets == 1)).sum().item()
    TN = ((all_preds == 0) & (all_targets == 0)).sum().item()
    FP = ((all_preds == 1) & (all_targets == 0)).sum().item()
    FN = ((all_preds == 0) & (all_targets == 1)).sum().item()

    accuracy = (TP + TN) / (TP + TN + FP + FN + 1e-8)
    precision = TP / (TP + FP + 1e-8)
    recall = TP / (TP + FN + 1e-8)
    f1 = 2 * precision * recall / (precision + recall + 1e-8)

    log["val_avg_loss"] = avg_loss
    log["accuracy"] = accuracy
    log["precision"] = precision
    log["recall"] = recall
    log["f1"] = f1

file_path = os.path.join('speeddating.csv')
df = pd.read_csv(file_path, na_values=['?'])
df = preprocess(df)
df = clear(df)
df['match'].value_counts(normalize=True)

cfg={
  "batch_size": 16,
  "max_workers": 2,
  "training": 0.7,
  "seed": 42,
  "n_in": None,
  "n_hidden": 12,
  "n_out": 1,
  "learning_rate": 0.0001,
  "epochs": 100,
  "dropout": 0.2,
  "weight_decay": 0.0001,
  "decision_threshold": 0.65
}

cfg["n_in"]=len(df.columns) - 1

wandb.init(project="speeddating", config=cfg)
trainer = Trainer(df)
trainer.fit()
torch.save(trainer.mlp.state_dict(), "mlp.pt")
wandb.save("mlp.pt")
wandb.finish()

sweep_cfg = {
    "method": "random",
    "metric": {"name": "val_loss", "goal": "minimize"},
    "parameters": {
        "n_hidden": {"values": [12, 16, 32]},
        "decision_threshold": {"values": [0.5, 0.6, 0.7]},
        "dropout": {"values": [0.2, 0.3]},
        "learning_rate": {"values": [0.1, 0.01, 0.001, 0.0001]}
    }
}

def sweep():
  wandb.init(config=cfg)
  trainer = Trainer(df)
  trainer.fit()
  torch.save(trainer.mlp.state_dict(), "mlp.pt")
  wandb.save("mlp.pt")
  wandb.finish()

sweep_id = wandb.sweep(sweep_cfg, project="speeddating")
wandb.agent(sweep_id, function=sweep, count=20)

